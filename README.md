# Machine-learning-assisted-graph-classification

# Data generation

The file 'initial_phases.txt' contains the MATLAB code to generate the initial phase values of the network. These values are homogeneously distributed between -pi and pi, and can be changed to vary between any -val to val as needed.

The file 'network_generation.py' contains Python 3 code to generate the following:
   1) The networks: Two types of networks can be generated here: ER random and scale-free. The networkx function erdos_renyi_graph() takes the network size (N), the probability  of two nodes being connected (N/k) and the seed value to randomize the generation of the graph. The seed value must be different for each graph so as to ensure that each        graph is unique. The function Barabasi_Albert_graph() takes in the network size (N), the number of connections to be made by each new node being connected to the graph (k/2)    (this graph is formed by an algorithm called preferential attachment) and the seed values. Again the seed value must be unique for each graph.
   2) The omega values being generated here are from the function randn() which takes in an argument N, which is the number of values to be generated, and then generates N values having mean 0 and variance 1. To change the mean or the variance to some other values, the following equation must be used:

values = sqrt(variance) * randn() + mean

Finally the file 'data_generation.m' is another MATLAB code for generating the final data that would be used as input for the machine learning model.
The code in this file is divided as following: 
  1) First the data such as size of the network, and the lambda values chosen are initialized.
  2) Then a loop runs for all the networks:
    a) The loop contains a function ode45 which is the inbuilt differential equation solver, using the 4th order Runge-Kutta method.
    b) The loop finally converts the theta values generated by the ode45 function to symbolic values and stores these in the appropriate files.
  3) There is a function called odefm that finds the theta-dot values at each time step using the Kuramoto differential equation.

The way the data is read depends on the required data. For example, use the readmatrix code inside the loop to read different omega values and initial phase values for each node.

# Training and testing the data

The data is of the shape (S, T, N) where T is the number of time-steps, N is the total number of nodes taken, and S is the number of samples. First the data is read from the file, and stored as numpy arrays in different variables using the np.load() function. The labels for the data are generated as (1, C) dimensional vectors, where C is the number of classes.
Thus for 2 class classification, the labels are [0 1] and [1 0].

The dataset is then split into training and testing sets in the ratio 8:2 respectively using the sklearn.model_selection function train_test_split()

x_train is the final input training data, x_test is the final input testing data.
y_train is the final training output label, and y_test is the final testing output label.

At this point the x_train and the x_test have the shape (0.8S, T, N) and (0.2S, T, N) but the input is required to be contained in a vector of length, thus the input is reshaped as (0.8S, T, N) to (0.8S, T, N, 1) using the function np.reshape().

The Model:

length_list is the list of lengths of time series to be used for training and testing the model.
node_list is the number of nodes to be used for training and testing the model.

We loop through the node_list and the length_list, and train and test the data according to the number of nodes in the ith position of the node_list and the lenght of time series in the jth position of length_list.

